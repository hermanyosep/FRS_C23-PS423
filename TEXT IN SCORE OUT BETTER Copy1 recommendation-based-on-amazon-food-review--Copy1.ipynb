{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\nfor dirname, _, filenames in os.walk('E:\\\\TFRS\\\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "#from sklearn.externals import joblib\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, LSTM\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "'''\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('E:\\\\TFRS\\\\'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "'''\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Import the data set\n",
    "df = pd.read_csv('Reviews.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName   \n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian  \\\n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time   \n",
       "0                     1                       1      5  1303862400  \\\n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns\n",
    "df = df.drop(['Id','Time','HelpfulnessNumerator','HelpfulnessDenominator','Summary'], axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>5</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>2</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>5</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>5</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>5</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ProductId          UserId              ProfileName  Score   \n",
       "568449  B001EO7N10  A28KG5XORO54AY         Lettie D. Carter      5  \\\n",
       "568450  B003S1WTCU  A3I8AFVPEE8KI5                R. Sawyer      2   \n",
       "568451  B004I613EE  A121AA1GQV751Z            pksd \"pk_007\"      5   \n",
       "568452  B004I613EE   A3IBEVCTXKNOH  Kathy A. Welch \"katwel\"      5   \n",
       "568453  B001LR2CU2  A3LGQPJCZVL9UC                 srfell17      5   \n",
       "\n",
       "                                                     Text  \n",
       "568449  Great for sesame chicken..this is a good if no...  \n",
       "568450  I'm disappointed with the flavor. The chocolat...  \n",
       "568451  These stars are small, so you can give 10-15 o...  \n",
       "568452  These are the BEST treats for training and rew...  \n",
       "568453  I am very satisfied ,product is as advertised,...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see few rows of the imported dataset\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows:  568454\n",
      "No of columns:  5\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows and columns\n",
    "rows, columns = df.shape\n",
    "print(\"No of rows: \", rows) \n",
    "print(\"No of columns: \", columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductId      object\n",
       "UserId         object\n",
       "ProfileName    object\n",
       "Score           int64\n",
       "Text           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values across columns-\n",
      " ProductId       0\n",
      "UserId          0\n",
      "ProfileName    26\n",
      "Score           0\n",
      "Text            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values present\n",
    "print('Number of missing values across columns-\\n', df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are no missing values with total records 568454\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>568454.0</td>\n",
       "      <td>4.183199</td>\n",
       "      <td>1.310436</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count      mean       std  min  25%  50%  75%  max\n",
       "Score  568454.0  4.183199  1.310436  1.0  4.0  5.0  5.0  5.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics of 'rating' variable\n",
    "df[['Score']].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum rating is: 1\n",
      "The maximum rating is: 5\n"
     ]
    }
   ],
   "source": [
    "# find minimum and maximum ratings \n",
    "\n",
    "def find_min_max_rating():\n",
    "    print('The minimum rating is: %d' %(df['Score'].min()))\n",
    "    print('The maximum rating is: %d' %(df['Score'].max()))\n",
    "    \n",
    "find_min_max_rating() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings are on scale of 1 - 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the distribution of ratings\n",
    "with sns.axes_style('white'):\n",
    "    g = sns.countplot(data=df, x=\"Score\")\n",
    "    g.set_ylabel(\"Total number of ratings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique USERS in Raw data =  218415\n",
      "Number of unique USERS in Raw data =  568454\n",
      "Number of unique ITEMS in Raw data =  5\n"
     ]
    }
   ],
   "source": [
    "# Number of unique user id and product id in the data\n",
    "print('Number of unique USERS in Raw data = ', df['ProfileName'].nunique())\n",
    "print('Number of unique USERS in Raw data = ', len(df['ProfileName']))\n",
    "print('Number of unique ITEMS in Raw data = ', df['Score'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take subset of dataset to make it less sparse/more dense. ( For example, keep the users only who has given 50 or more number of ratings )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "5    363122\n",
       "4     80655\n",
       "1     52268\n",
       "3     42640\n",
       "2     29769\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 users based on rating\n",
    "most_rated = df.groupby('Score').size().sort_values(ascending=False)[:5]\n",
    "most_rated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data model preparation as per requirement on number of minimum ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['ProfileName'].value_counts()\n",
    "df_final = df[df['ProfileName'].isin(counts[counts >= 10].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProfileName\n",
       "C. F. Hill \"CFH\"                               451\n",
       "O. Brown \"Ms. O. Khannah-Brown\"                421\n",
       "Gary Peterson                                  389\n",
       "Rebecca of Amazon \"The Rebecca Review\"         365\n",
       "Chris                                          363\n",
       "                                              ... \n",
       "Steven Wolff                                     1\n",
       "joycomeau                                        1\n",
       "Lizz                                             1\n",
       "Phyllis A. De Smet-Howard \"tweedsmerewillo\"      1\n",
       "srfell17                                         1\n",
       "Name: count, Length: 218415, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>2</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B0009XLVG0</td>\n",
       "      <td>A2725IB4YY9JEB</td>\n",
       "      <td>A Poeng \"SparkyGoHome\"</td>\n",
       "      <td>5</td>\n",
       "      <td>One of my boys needed to lose some weight and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B0009XLVG0</td>\n",
       "      <td>A327PCT23YH90</td>\n",
       "      <td>LT</td>\n",
       "      <td>1</td>\n",
       "      <td>My cats have been happily eating Felidae Plati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A18ECVX2RJ7HUE</td>\n",
       "      <td>willie \"roadie\"</td>\n",
       "      <td>4</td>\n",
       "      <td>good flavor! these came securely packed... the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>A2MUGFV2TDQ47K</td>\n",
       "      <td>Lynrie \"Oh HELL no\"</td>\n",
       "      <td>5</td>\n",
       "      <td>The Strawberry Twizzlers are my guilty pleasur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ProductId          UserId             ProfileName  Score   \n",
       "3   B000UA0QIQ  A395BORC6FGVXV                    Karl      2  \\\n",
       "11  B0009XLVG0  A2725IB4YY9JEB  A Poeng \"SparkyGoHome\"      5   \n",
       "12  B0009XLVG0   A327PCT23YH90                      LT      1   \n",
       "13  B001GVISJM  A18ECVX2RJ7HUE         willie \"roadie\"      4   \n",
       "14  B001GVISJM  A2MUGFV2TDQ47K     Lynrie \"Oh HELL no\"      5   \n",
       "\n",
       "                                                 Text  \n",
       "3   If you are looking for the secret ingredient i...  \n",
       "11  One of my boys needed to lose some weight and ...  \n",
       "12  My cats have been happily eating Felidae Plati...  \n",
       "13  good flavor! these came securely packed... the...  \n",
       "14  The Strawberry Twizzlers are my guilty pleasur...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who have rated 50 or more items = 190432\n",
      "Number of unique USERS in final data =  8830\n",
      "Number of unique ITEMS in final data =  5\n"
     ]
    }
   ],
   "source": [
    "print('Number of users who have rated 50 or more items =', len(df_final))\n",
    "print('Number of unique USERS in final data = ', df_final['ProfileName'].nunique())\n",
    "print('Number of unique ITEMS in final data = ', df_final['Score'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data randomly into train and test dataset. ( For example split it in 70/30 ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ProductId          UserId   \n",
      "473294  B0030VJ79Q  A1SR28GSP1AJ8K  \\\n",
      "31089   B007M83302   AJT8SZ11T4EYP   \n",
      "406440  B005T12ZVW  A19UTUEBWKIZFT   \n",
      "327276  B004NM9YW6   A3SG0KEQBXK9O   \n",
      "112351  B002O7BAB0  A3FJFCAEF7Q1W4   \n",
      "\n",
      "                                             ProfileName  Score   \n",
      "473294                                          Mom2JPC3      5  \\\n",
      "31089                                        Jane Austin      5   \n",
      "406440                                        Bernadette      5   \n",
      "327276                                             Bruce      4   \n",
      "112351  Cmonkey \"Scuba Diving Photographing Dog Spoil...      5   \n",
      "\n",
      "                                                     Text  \n",
      "473294  These pouches are a godsend. I keep a stockpil...  \n",
      "31089   I absolutely love these chips.  All the flavor...  \n",
      "406440  Does it make me smarter? Nope. Does it taste f...  \n",
      "327276  The beauty of the Keurig machine is that you c...  \n",
      "112351  Lamb is great for sensitive stomachs and this ...  \n"
     ]
    }
   ],
   "source": [
    "#Split the training and test data in the ratio 70:30\n",
    "train_data, test_data = train_test_split(df_final, test_size = 0.1, random_state=0)\n",
    "\n",
    "print(train_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape:  (19044, 5)\n",
      "Train data shape:  (171388, 5)\n"
     ]
    }
   ],
   "source": [
    "def shape():\n",
    "    print(\"Test data shape: \", test_data.shape)\n",
    "    print(\"Train data shape: \", train_data.shape)\n",
    "shape() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######BISA DARI TEXT PREDICT SCORE TP BELUM PER USER\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Prepare the training data\n",
    "x_train = train_data['ProfileName']\n",
    "y_train = train_data['Score']\n",
    "\n",
    "# Prepare the test data\n",
    "x_test = test_data['ProfileName']0\n",
    "y_test = test_data['Score']\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "true_train = tokenizer.texts_to_sequences(x_train)\n",
    "true_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = 200\n",
    "\n",
    "true_train2 = pad_sequences(true_train, maxlen=max_length, padding='post')\n",
    "true_test2 = pad_sequences(true_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# Scale the ratings between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "y_train = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(true_train2, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "'''\n",
    "# Predict the ratings for the test set\n",
    "predictions = model.predict(true_test2)\n",
    "\n",
    "# Inverse transform the scaled ratings to get the actual ratings\n",
    "predicted_ratings = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Create a DataFrame with the predicted ratings and original ratings\n",
    "predicted_df = pd.DataFrame({\n",
    "    'PredictedRating': predicted_ratings.flatten(),\n",
    "    'OriginalRating': y_test.values.flatten()\n",
    "})\n",
    "\n",
    "print(predicted_df.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define custom scaling function\n",
    "def custom_scaling(data, min_value, max_value):\n",
    "    # Compute the minimum and maximum values of the data\n",
    "    data_min = np.min(data)\n",
    "    data_max = np.max(data)\n",
    "    \n",
    "    # Scale the data using linear transformation\n",
    "    #scaled_data = (data - data_min) * (max_value - min_value) / (data_max - data_min) + min_value\n",
    "    scaled_data = (data - data_min) / (data_max - data_min)\n",
    "    return scaled_data\n",
    "\n",
    "def inverse_custom_scaling(scaled_data, min_value, max_value):\n",
    "    # Compute the minimum and maximum values of the scaled data range\n",
    "    scaled_min = np.min(scaled_data)\n",
    "    scaled_max = np.max(scaled_data)\n",
    "    \n",
    "    # Reverse the scaling using the inverse linear transformation\n",
    "    #data = (scaled_data - min_value) * (scaled_max - scaled_min) / (max_value - min_value) + scaled_min\n",
    "    data = scaled_data * (max_value - min_value) + min_value\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "data = np.array([2, 5, 8, 12, 15])  # Sample data\n",
    "scaled_data = custom_scaling(data, 0, 1)  # Custom scaling between 0 and 1\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Scaled Data:\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "######BISA DARI PROFILE NAME PREDICT SCORE TP BELUM PER USER\n",
    "#### APA BISA X nya 2 buah?\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,MaxAbsScaler,Normalizer,PowerTransformer\n",
    "\n",
    "# Prepare the training data\n",
    "#x1_train = train_data['ProfileName'] #check data size first PLS !\n",
    "x1_train = train_data['UserId'] \n",
    "x2_train = train_data['ProductId']\n",
    "y_train = train_data['Score']\n",
    "\n",
    "# Prepare the test data\n",
    "#x1_test = test_data['ProfileName']\n",
    "x1_test = test_data['UserId']\n",
    "x2_test = test_data['ProductId']\n",
    "y_test = test_data['Score']\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x1_train)\n",
    "tokenizer.fit_on_texts(x2_train)\n",
    "\n",
    "true_train1 = tokenizer.texts_to_sequences(x1_train)\n",
    "true_train2 = tokenizer.texts_to_sequences(x2_train)\n",
    "true_test1 = tokenizer.texts_to_sequences(x1_test)\n",
    "true_test2 = tokenizer.texts_to_sequences(x2_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = 50\n",
    "\n",
    "true_trainX = pad_sequences(true_train1, maxlen=max_length, padding='post')\n",
    "true_trainX2 = pad_sequences(true_train2, maxlen=max_length, padding='post')\n",
    "true_testX = pad_sequences(true_test1, maxlen=max_length, padding='post')\n",
    "true_testX2 = pad_sequences(true_test2, maxlen=max_length, padding='post')\n",
    "\n",
    "x_train = np.concatenate((true_trainX, true_trainX2), axis=1)\n",
    "x_test = np.concatenate((true_testX, true_testX2), axis=1)\n",
    "\n",
    "# Scale the ratings between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "y_train2 = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "#y_train2 = custom_scaling(y_train.values.reshape(-1,1),0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_trainX2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######BISA DARI TEXT PREDICT SCORE TP BELUM PER USER\n",
    "#### APA BISA X nya 2 buah?\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,MaxAbsScaler,Normalizer,PowerTransformer\n",
    "\n",
    "# Prepare the training data\n",
    "x1_train = train_data['Text'] #check data size first PLS !\n",
    "x2_train = train_data['ProductId']\n",
    "y_train = train_data['Score']\n",
    "\n",
    "# Prepare the test data\n",
    "x1_test = test_data['Text']\n",
    "x2_test = test_data['ProductId']\n",
    "y_test = test_data['Score']\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x1_train)\n",
    "tokenizer.fit_on_texts(x2_train)\n",
    "\n",
    "true_train1 = tokenizer.texts_to_sequences(x1_train)\n",
    "true_train2 = tokenizer.texts_to_sequences(x2_train)\n",
    "true_test1 = tokenizer.texts_to_sequences(x1_test)\n",
    "true_test2 = tokenizer.texts_to_sequences(x2_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = 50\n",
    "\n",
    "true_trainX = pad_sequences(true_train1, maxlen=max_length, padding='post')\n",
    "true_trainX2 = pad_sequences(true_train2, maxlen=max_length, padding='post')\n",
    "true_testX = pad_sequences(true_test1, maxlen=max_length, padding='post')\n",
    "true_testX2 = pad_sequences(true_test2, maxlen=max_length, padding='post')\n",
    "\n",
    "x_train = np.concatenate((true_trainX, true_trainX2), axis=1)\n",
    "x_test = np.concatenate((true_testX, true_testX2), axis=1)\n",
    "\n",
    "# Scale the ratings between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "y_train2 = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "#y_train2 = custom_scaling(y_train.values.reshape(-1,1),0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######BISA DARI TEXT PREDICT SCORE TP BELUM PER USER\n",
    "#### APA BISA X nya 2 buah?\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,MaxAbsScaler,Normalizer,PowerTransformer\n",
    "\n",
    "# Prepare the training data\n",
    "x1_train = train_data['ProfileName'] #check data size first PLS !\n",
    "x2_train = train_data['ProductId']\n",
    "x3_train = train_data['Text']\n",
    "y_train = train_data['Score']\n",
    "\n",
    "# Prepare the test data\n",
    "x1_test = test_data['ProfileName']\n",
    "x2_test = test_data['ProductId']\n",
    "x3_test = train_data['Text']\n",
    "y_test = test_data['Score']\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x1_train)\n",
    "tokenizer.fit_on_texts(x2_train)\n",
    "tokenizer.fit_on_texts(x3_train)\n",
    "tokenizer.fit_on_texts(x1_test)\n",
    "tokenizer.fit_on_texts(x2_test)\n",
    "tokenizer.fit_on_texts(x3_test)\n",
    "\n",
    "true_train1 = tokenizer.texts_to_sequences(x1_train)\n",
    "true_train2 = tokenizer.texts_to_sequences(x2_train)\n",
    "true_train3 = tokenizer.texts_to_sequences(x3_train)\n",
    "true_test1 = tokenizer.texts_to_sequences(x1_test)\n",
    "true_test2 = tokenizer.texts_to_sequences(x2_test)\n",
    "true_test3 = tokenizer.texts_to_sequences(x3_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = 50\n",
    "\n",
    "true_trainX = pad_sequences(true_train1, maxlen=max_length, padding='post')\n",
    "true_trainX2 = pad_sequences(true_train2, maxlen=max_length, padding='post')\n",
    "true_trainX3 = pad_sequences(true_train3, maxlen=max_length, padding='post')\n",
    "true_testX = pad_sequences(true_test1, maxlen=max_length, padding='post')\n",
    "true_testX2 = pad_sequences(true_test2, maxlen=max_length, padding='post')\n",
    "true_testX3 = pad_sequences(true_test3, maxlen=max_length, padding='post')\n",
    "\n",
    "x_train = np.concatenate((true_trainX, true_trainX2), axis=1)\n",
    "x_train2 = np.concatenate((x_train, true_trainX3), axis=1)\n",
    "#x_test = np.concatenate((true_testX, true_testX2), axis=1)\n",
    "#x_test2 = np.concatenate((x_test, true_testX3), axis=1)\n",
    "\n",
    "# Scale the ratings between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "y_train2 = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "#y_train2 = custom_scaling(y_train.values.reshape(-1,1),0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4285/4285 [==============================] - 27s 6ms/step - loss: 0.1034 - accuracy: 0.6060 - val_loss: 0.0989 - val_accuracy: 0.6036\n",
      "Epoch 2/10\n",
      "4285/4285 [==============================] - 26s 6ms/step - loss: 0.1007 - accuracy: 0.6073 - val_loss: 0.0995 - val_accuracy: 0.6036\n",
      "Epoch 3/10\n",
      "4285/4285 [==============================] - 26s 6ms/step - loss: 0.0999 - accuracy: 0.6073 - val_loss: 0.0990 - val_accuracy: 0.6036\n",
      "Epoch 4/10\n",
      "4285/4285 [==============================] - 26s 6ms/step - loss: 0.0995 - accuracy: 0.6073 - val_loss: 0.0989 - val_accuracy: 0.6036\n",
      "Epoch 5/10\n",
      "4285/4285 [==============================] - 26s 6ms/step - loss: 0.0992 - accuracy: 0.6073 - val_loss: 0.0991 - val_accuracy: 0.6036\n",
      "Epoch 6/10\n",
      "4285/4285 [==============================] - 26s 6ms/step - loss: 0.0990 - accuracy: 0.6073 - val_loss: 0.0989 - val_accuracy: 0.6036\n",
      "Epoch 7/10\n",
      "4285/4285 [==============================] - 26s 6ms/step - loss: 0.0989 - accuracy: 0.6073 - val_loss: 0.0989 - val_accuracy: 0.6036\n",
      "Epoch 8/10\n",
      "4285/4285 [==============================] - 26s 6ms/step - loss: 0.0989 - accuracy: 0.6073 - val_loss: 0.0989 - val_accuracy: 0.6036\n",
      "Epoch 9/10\n",
      "4285/4285 [==============================] - 26s 6ms/step - loss: 0.0989 - accuracy: 0.6073 - val_loss: 0.0990 - val_accuracy: 0.6036\n",
      "Epoch 10/10\n",
      "4285/4285 [==============================] - 26s 6ms/step - loss: 0.0989 - accuracy: 0.6073 - val_loss: 0.0989 - val_accuracy: 0.6036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Predict the ratings for the test set\\npredictions = model.predict(true_test2)\\n\\n# Inverse transform the scaled ratings to get the actual ratings\\npredicted_ratings = scaler.inverse_transform(predictions)\\n\\n# Create a DataFrame with the predicted ratings and original ratings\\npredicted_df = pd.DataFrame({\\n    'PredictedRating': predicted_ratings.flatten(),\\n    'OriginalRating': y_test.values.flatten()\\n})\\n\\nprint(predicted_df.head())\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train[:,:50], y_train2, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "'''\n",
    "# Predict the ratings for the test set\n",
    "predictions = model.predict(true_test2)\n",
    "\n",
    "# Inverse transform the scaled ratings to get the actual ratings\n",
    "predicted_ratings = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Create a DataFrame with the predicted ratings and original ratings\n",
    "predicted_df = pd.DataFrame({\n",
    "    'PredictedRating': predicted_ratings.flatten(),\n",
    "    'OriginalRating': y_test.values.flatten()\n",
    "})\n",
    "\n",
    "print(predicted_df.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict the ratings for the test set\n",
    "predictions = model.predict(x_test[:,:50])\n",
    "\n",
    "# Inverse transform the scaled ratings to get the actual ratings\n",
    "predicted_ratings = scaler.inverse_transform(predictions)\n",
    "#predicted_ratings = inverse_custom_scaling(predictions,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'UserId' and 'ProductId' to data arrays\n",
    "name_idx = train_data['ProfileName'].values\n",
    "product_idx = train_data['ProductId'].values\n",
    "\n",
    "# Create a new DataFrame with converted data arrays\n",
    "df_converted = pd.DataFrame({'ProfileName': name_idx, 'ProductId': product_idx, 'Score': 0})\n",
    "\n",
    "# Create pivot table with the converted DataFrame\n",
    "final_ratings_matrix = pd.pivot_table(df_converted, index='ProfileName', columns='ProductId', values='Score')\n",
    "final_ratings_matrix.fillna(0, inplace=True)\n",
    "\n",
    "# Print the final_ratings_matrix\n",
    "final_ratings_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'final_ratings_matrix' is a pandas DataFrame containing the pivot table\n",
    "# obtained from pd.pivot_table()\n",
    "\n",
    "# Fill NaN values with 0\n",
    "final_ratings_matrix.fillna(0, inplace=True)\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "array3 = final_ratings_matrix.reset_index().melt(id_vars=['ProfileName'], value_vars=final_ratings_matrix.columns)\n",
    "array3.columns = ['ProfileName', 'ProductId', 'Score']\n",
    "array3 = array3[['ProfileName', 'ProductId']].values\n",
    "\n",
    "# Print the resulting array\n",
    "print(type(array3))\n",
    "print(array3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARRAY TEST UNTUK TEXT\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'final_ratings_matrix' is a pandas DataFrame containing the pivot table\n",
    "# obtained from pd.pivot_table()\n",
    "\n",
    "# Fill NaN values with 0\n",
    "final_ratings_matrix.fillna(0, inplace=True)\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "array3 = final_ratings_matrix.reset_index().melt(id_vars=['ProfileName'], value_vars=final_ratings_matrix.columns)\n",
    "array3.columns = ['ProfileName', 'ProductId', 'Score']\n",
    "#array3 = array3[['ProfileName', 'ProductId']].values\n",
    "\n",
    "# Print the resulting array\n",
    "print(type(array3))\n",
    "print(array3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict the ratings for the test set\n",
    "ratings = model.predict(array3)\n",
    "\n",
    "# Inverse transform the scaled ratings to get the actual ratings\n",
    "predicted_ratings = scaler.inverse_transform(ratings)\n",
    "\n",
    "# Create a new DataFrame with the columns from 'array3' and the 'predicted_ratings'\n",
    "df_predict = pd.DataFrame(array3, columns=['ProfileName', 'ProductId'])\n",
    "df_predict['PredictedRating'] = predicted_ratings\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.to_csv('ratingpredict_resulted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the model accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Prepare the training data\n",
    "x_train = train_data['ProfileName'].unique()\n",
    "y_train = train_data['Score']\n",
    "\n",
    "# Prepare the test data\n",
    "x_test = test_data['ProfileName']\n",
    "y_test = test_data['Score']\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "true_train = tokenizer.texts_to_sequences(x_train)\n",
    "true_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = 200\n",
    "\n",
    "true_train2 = pad_sequences(true_train, maxlen=max_length, padding='post')\n",
    "true_test2 = pad_sequences(true_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# Scale the ratings between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "y_train = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(true_train2, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Predict the ratings for the test set\n",
    "predictions = model.predict(true_test2)\n",
    "\n",
    "# Inverse transform the scaled ratings to get the actual ratings\n",
    "predicted_ratings = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Create a DataFrame with the predicted ratings and original ratings\n",
    "predicted_df = pd.DataFrame({\n",
    "    'PredictedRating': predicted_ratings.flatten(),\n",
    "    'OriginalRating': y_test.values.flatten()\n",
    "})\n",
    "\n",
    "print(predicted_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####USING BERT PRETRAINED\n",
    "# Load the BERT model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "# Load the pretrained DistilBERT model for sequence classification\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set the input shapes\n",
    "input_ids = Input(shape=(max_length,), dtype=tf.int64)\n",
    "attention_mask = Input(shape=(max_length,), dtype=tf.int64)\n",
    "\n",
    "# Get the BERT output\n",
    "bert_output = model(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "# Create the model architecture\n",
    "dense_layer = Dense(128, activation='relu')(bert_output)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "output_layer = Dense(1, activation='linear')(dropout_layer)\n",
    "\n",
    "# Combine the input and output layers into a model\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([true_trainX, true_trainX2], y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####USING XLNET PRETRAINED\n",
    "# Load the BERT model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFXLNetModel\n",
    "\n",
    "xlnet_model = TFXLNetModel.from_pretrained('xlnet-base-uncased')\n",
    "\n",
    "# Set the input shapes\n",
    "input_ids = Input(shape=(max_length,), dtype=tf.int64)\n",
    "attention_mask = Input(shape=(max_length,), dtype=tf.int64)\n",
    "\n",
    "# Get the BERT output\n",
    "xlnet_output = xlnet_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "# Create the model architecture\n",
    "model = tf.keras.Sequential()\n",
    "model.add(xlnet_output)\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "# Combine the input and output layers into a model\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([true_trainX, true_trainX2], y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####USING BERT PRETRAINED\n",
    "# Load the BERT model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFBertModel\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the input shapes\n",
    "input_ids = Input(shape=(max_length,), dtype=tf.int32)\n",
    "attention_mask = Input(shape=(max_length,), dtype=tf.int32)\n",
    "\n",
    "# Get the BERT output\n",
    "bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "# Create the model architecture\n",
    "dense_layer = Dense(128, activation='relu')(bert_output)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "output_layer = Dense(1, activation='linear')(dropout_layer)\n",
    "\n",
    "# Combine the input and output layers into a model\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([true_trainX, true_trainX2], y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input shape for the base model\n",
    "input_ids = Input(shape=(max_seq_length,), dtype=tf.int32)\n",
    "attention_mask = Input(shape=(max_seq_length,), dtype=tf.int32)\n",
    "\n",
    "# Pass the inputs through the base model\n",
    "bert_output = base_model(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "# Add additional layers on top of the BERT output\n",
    "x = Dense(128, activation='relu')(bert_output)\n",
    "output = Dense(1, activation='linear')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = 'saved_model/1'\n",
    "tf.saved_model.save(model, export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POST TRAIN QUANTIZATION\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "tflite_model_file = pathlib.Path(export_dir+'/recommender_model.tflite')\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(export_dir+'/recommender_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfrs",
   "language": "python",
   "name": "tfrs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
