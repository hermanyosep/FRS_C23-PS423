{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tqdm.notebook as tqdm # progress bars\n",
    "import implicit # Fast, sparse ALS implementation\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Preprocessing\n",
    "'''\n",
    "\n",
    "def safe_train_test_split(users, items, test_size=0.25, random_state=0):\n",
    "    '''\n",
    "    Performs a train test split on interactions, guaranteeing that every user \n",
    "    will be represented in both the train and the test split (assuming each user appears twice).\n",
    "    Note that sklearn's train_test_split function, even when stratified by user does not guarantee this,\n",
    "    unless the test_size is such that every user should be in both splits (0.5 if only 2 reviews/user)\n",
    "    \n",
    "    users is a list of user_ids\n",
    "    items is a list of items_ids\n",
    "    test_size is the approximate proportion of samples to be considered \"test samples\"\n",
    "    \n",
    "    The indices of users and items should correlate to represent all positive interactions\n",
    "    '''\n",
    "    \n",
    "    user_train = []\n",
    "    user_test = []\n",
    "    \n",
    "    item_train = []\n",
    "    item_test = []\n",
    "    \n",
    "    random.seed(random_state)\n",
    "    \n",
    "    for user_id in tqdm.tqdm(users.unique()):\n",
    "        examples = list(users[users.values == user_id].index)\n",
    "        random.shuffle(examples)\n",
    "        \n",
    "        # Not enough samples to perform proper split: pull one into test, then throw the rest in train\n",
    "        if len(examples) * test_size < 1:\n",
    "            user_test.append(user_id)\n",
    "            item_test.append(items[examples[0]])\n",
    "            \n",
    "            for i in range(1, len(examples)):\n",
    "                user_train.append(user_id)\n",
    "                item_train.append(items[examples[i]])\n",
    "            \n",
    "        # Enough to perform proper split: throw into train and test according to test_size\n",
    "        else:\n",
    "            test_samples = int(len(examples) * test_size)\n",
    "            \n",
    "            for i in range(0, test_samples):\n",
    "                user_test.append(user_id)\n",
    "                item_test.append(items[examples[i]])\n",
    "                \n",
    "            for j in range(test_samples, len(examples)):\n",
    "                user_train.append(user_id)\n",
    "                item_train.append(items[examples[j]])\n",
    "            \n",
    "    return user_train, user_test, item_train, item_test\n",
    "\n",
    "def make_recipe_ingr_xref(recipes):\n",
    "    '''\n",
    "    recipes is the pandas dataframe for preprceossed recipes. The ingredient_ids column should store lists of integers\n",
    "    \n",
    "    Returns new pandas dataframe, a cross-reference table for all recipes and ingredients\n",
    "    '''\n",
    "    \n",
    "    recipe_ids = []\n",
    "    ingr_ids = []\n",
    "    \n",
    "    for row in tqdm.tqdm(recipes['ingredient_ids'].index):\n",
    "        for ingr_id in recipes['ingredient_ids'][row]:\n",
    "            recipe_ids.append(recipes.loc[row, 'i'])\n",
    "            ingr_ids.append(ingr_id)\n",
    "            \n",
    "    return pd.DataFrame.from_dict({'i': recipe_ids, 'ingr': ingr_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Reccomendation Generation\n",
    "'''\n",
    "\n",
    "def get_positive_interactions(interactions, user_id):\n",
    "    '''\n",
    "    interactions is a pandas dataframe describing all known positive interactions, for filtering purposes\n",
    "    user_id is an integer specifying the user for which to pull the positive interactions\n",
    "    \n",
    "    returns a list of recipe_ids, specifying the recipes the given user has interacted with\n",
    "    '''\n",
    "    \n",
    "    return interactions[interactions['u'] == user_id]['i'].values\n",
    "\n",
    "def get_reccomendations(predictions, k, filtered_recipe_ids=[], filter_val=-100):\n",
    "    '''\n",
    "    General purpose function for pulling reccomendations based on a value for each recipe\n",
    "    \n",
    "    predictions is a numpy array of length num_recipes, that assigns a value to each recipe\n",
    "    k is an integer, the number of reccomendations to generate\n",
    "    filtered_recipe_ids is a list of recipe_ids, specifying which recipes should not be included in the reccomendations\n",
    "    filter_val is the value that replaces filtered recipe_ids for filtering purposes\n",
    "    \n",
    "    Returns recipe_ids, a list of the top k reccomended recipes, with index 0 being the most reccomended\n",
    "    '''\n",
    "    \n",
    "    # Replace all recipes to be filtered out with the filter value, which should be a lower value\n",
    "    predictions[filtered_recipe_ids] = filter_val\n",
    "    \n",
    "    # Get the top k reccomendations for the user (NOTE: these are not in order)\n",
    "    unsorted_recs = np.argpartition(predictions, -k)[-k:]\n",
    "    \n",
    "    # Sort the reccomendations by their predictions values, in descending order\n",
    "    sorted_recs = sorted(unsorted_recs, key=lambda rec: predictions[rec], reverse=True)\n",
    "\n",
    "    return sorted_recs\n",
    "\n",
    "def collab_filter_reccomendations(item_embedding, user_embedding, user_id, k, interactions=None, filter_recs=False):\n",
    "    '''\n",
    "    item embedding is of shape (items, features) describing the features for each item\n",
    "    user_embedding is of shape (users, features) describing the features for each user\n",
    "    user_id is an integer specifying the user to get reccomendations for\n",
    "    k is an integer specifying the number of reccomendations to pull\n",
    "    interactions is a pandas dataframe describing all known positive interactions, for filtering purposes\n",
    "        Note: interactions is only required if filter_recs is true\n",
    "    filter_recs is a boolean, specifying whether or not to filter known positive interactions specified in interactions\n",
    "    \n",
    "    Returns a list of recipe_ids similar to the general-purpose get_reccomendations function\n",
    "    '''\n",
    "    \n",
    "    predictions = item_embedding @ user_embedding[user_id, :]\n",
    "    \n",
    "    filtered_recipe_ids = get_positive_interactions(interactions, user_id) if filter_recs else []\n",
    "\n",
    "    return get_reccomendations(predictions, k, filtered_recipe_ids)\n",
    "\n",
    "def collab_filter_train_test_matrix(item_embedding, user_embedding, k, interactions):\n",
    "    '''\n",
    "    parameters are defined as for collab_filter_reccomendations\n",
    "    \n",
    "    Returns a tuple (train_rec_matrix, test_rec_matrix) where both are matrices of size (num_users, k) providing\n",
    "        the top k reccomendations for each user (most reccomended at index 0). train_rec_matrix does not filter known\n",
    "        positive interactions, while test_rec_matrix does filter known positive interactions\n",
    "    '''\n",
    "    \n",
    "    train_rec_matrix = []\n",
    "    test_rec_matrix = []\n",
    "    \n",
    "    for user_id in tqdm.tqdm(range(user_embedding.shape[0])):\n",
    "        predictions = item_embedding @ user_embedding[user_id, :]\n",
    "        filtered_recipe_ids = get_positive_interactions(interactions, user_id)\n",
    "        \n",
    "        train_recs = get_reccomendations(predictions, k)\n",
    "        test_recs = get_reccomendations(predictions, k, filtered_recipe_ids)\n",
    "\n",
    "        train_rec_matrix.append(train_recs)\n",
    "        test_rec_matrix.append(test_recs)\n",
    "\n",
    "    return np.array(train_rec_matrix), np.array(test_rec_matrix)\n",
    "\n",
    "def similar_recipes(embeddings, recipe_id, k):\n",
    "    '''\n",
    "    Embeddings is a matrix of shape (num_recipes, num_feature)\n",
    "    recipe_id is an integer, the recipe to find similar recipes to\n",
    "    \n",
    "    Note that embeddings could be tf-idf embeddings to produce content-based reccomendations (\"similar recipes\")\n",
    "        OR embeddings could be item_embeddings generated from collaborative filtering (\"customers who liked this recipe also liked\")\n",
    "    \n",
    "    Returns a list of recipe_ids, the most similar recipes to the given recipe\n",
    "    '''\n",
    "    \n",
    "    # Values for each recipe based on similarity to given recipe\n",
    "    similarities = cosine_similar_predictions(embeddings, embeddings[recipe_id])\n",
    "    \n",
    "    # Get reccomendations, filtering out the queries recipe\n",
    "    return get_reccomendations(similarities, k, filtered_recipe_ids=[recipe_id])\n",
    "\n",
    "def mean_profile_reccomendations(embeddings, user_id, k, interactions, filter_recs=False):\n",
    "    '''\n",
    "    Parameters defined as above\n",
    "    \n",
    "    Returns a list of recipe_ids, reccomended based on the mean embedding of all recipes the user has interacted with\n",
    "    '''\n",
    "    \n",
    "    interacted_ids = get_positive_interactions(interactions, user_id)\n",
    "    \n",
    "    interacted_embeddings = embeddings[interacted_ids]\n",
    "\n",
    "    average_embedding = np.mean(interacted_embeddings, axis=0)\n",
    "\n",
    "    similarities = cosine_similar_predictions(embeddings, average_embedding)\n",
    "\n",
    "    return get_reccomendations(similarities, k, filtered_recipe_ids=interacted_ids if filter_recs else [])\n",
    "\n",
    "def mean_profile_train_test_matrix(embeddings, k, interactions):\n",
    "    '''\n",
    "    Produces a matrix of size (num_users, k) providing the top k reccomendations for each user\n",
    "    \n",
    "    This takes like 12 minutes to run with tf_idf embeddings, I think because of the ~8000 features\n",
    "    This takes 6 minutes for collab_filter embeddings, with only 8 features. Probably longer b/c it normalizes similarities\n",
    "    '''\n",
    "    \n",
    "    train_rec_matrix = []\n",
    "    test_rec_matrix = []\n",
    "    \n",
    "    for user_id in tqdm.tqdm(range(max(interactions['u']) + 1)):\n",
    "    #for user_id in tqdm.tqdm(range(10)):\n",
    "        interacted_ids = get_positive_interactions(interactions, user_id)\n",
    "        interacted_embeddings = embeddings[interacted_ids]\n",
    "\n",
    "        average_embedding = np.mean(interacted_embeddings, axis=0)\n",
    "        similarities = cosine_similar_predictions(embeddings, average_embedding)\n",
    "\n",
    "        train_recs = get_reccomendations(similarities, k)\n",
    "        test_recs = get_reccomendations(similarities, k, filtered_recipe_ids=interacted_ids)\n",
    "\n",
    "        train_rec_matrix.append(train_recs)\n",
    "        test_rec_matrix.append(test_recs)\n",
    "        \n",
    "    return np.array(train_rec_matrix), np.array(test_rec_matrix)\n",
    "\n",
    "def random_rec_matrix(num_users, num_recipes, k):\n",
    "    '''\n",
    "    Function to create a dummy reccomendation matrix, as a baseline for other models\n",
    "    '''\n",
    "    \n",
    "    rec_matrix = []\n",
    "    \n",
    "    for user_id in tqdm.tqdm(range(num_users)):\n",
    "        rec_matrix.append([random.randint(0, num_recipes - 1) for i in range(k)])\n",
    "        \n",
    "    return np.array(rec_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_rec_matrix, test_rec_matrix = mean_profile_train_test_matrix(model.item_factors, 10, train_interactions)\\nprint(mean_precision_at_k(train_matrix, positive_train_interactions, train_rec_matrix, adjusted=True)) # 0.222729\\nprint(mean_precision_at_k(test_matrix, positive_test_interactions, test_rec_matrix, adjusted=True)) # 0.00817\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(mean_profile_reccomendations(tf_idf_embeddings, 0, 20, train_interactions))\n",
    "#print(mean_profile_reccomendations(model.item_factors, 0, 20, train_interactions))\n",
    "#print(collab_filter_reccomendations(model.item_factors, model.user_factors, 0, 20, train_interactions))\n",
    "#print(train_interactions[train_interactions['u'] == 0]['i'].values)\n",
    "\n",
    "# Performance suggests this method is poor at generalizing to new recipes...\n",
    "'''\n",
    "train_rec_matrix, test_rec_matrix = mean_profile_train_test_matrix(tf_idf_embeddings, 10, train_interactions)\n",
    "print(mean_precision_at_k(train_matrix, positive_train_interactions, train_rec_matrix, adjusted=True)) # 0.597\n",
    "print(mean_precision_at_k(test_matrix, positive_test_interactions, test_rec_matrix, adjusted=True)) # 0.001286\n",
    "'''\n",
    "\n",
    "# Performance also suggests poor generalization, but better generalization than tf_idf reccomendations\n",
    "'''\n",
    "train_rec_matrix, test_rec_matrix = mean_profile_train_test_matrix(model.item_factors, 10, train_interactions)\n",
    "print(mean_precision_at_k(train_matrix, positive_train_interactions, train_rec_matrix, adjusted=True)) # 0.222729\n",
    "print(mean_precision_at_k(test_matrix, positive_test_interactions, test_rec_matrix, adjusted=True)) # 0.00817\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Similarity Measurements (Content-Based Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Content-based filtering utilities\n",
    "'''\n",
    "\n",
    "def get_tf_idf_embeddings(recipe_ingr_xref):\n",
    "    '''\n",
    "    recipe_ingr_xref is a pandas Dataframe, like the one returned from makr_recipe_ingr_xref\n",
    "    \n",
    "    returns a sparse matrix of tf-idf embeddings for each recipe\n",
    "    ''' \n",
    "    \n",
    "    # Create the document frequency matrix\n",
    "    ingr_freq = csr_matrix((np.ones(len(recipe_ingr_xref)), (recipe_ingr_xref['i'], recipe_ingr_xref['ingr'])))\n",
    "    \n",
    "    # Generate the tf-idf embeddings from the document frequency matrix\n",
    "    tf = TfidfTransformer()\n",
    "    tf_idf_embeddings = tf.fit_transform(ingr_freq) \n",
    "    \n",
    "    return tf_idf_embeddings\n",
    "\n",
    "def cosine_similar_predictions(embeddings, query):\n",
    "    '''\n",
    "    embeddings is a matrix of shape (num_recipes, num_features)\n",
    "    query is a list of length num_features\n",
    "    '''\n",
    "    \n",
    "    return cosine_similarity(embeddings, query.reshape(1, -1)).squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34f71ba71b7462aa66457f3b74ae3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "recipes = pd.read_csv(r'E:\\TFRS\\food.com recipes and interactions\\PP_recipes.csv')\n",
    "ingr_map = pd.read_pickle(r'E:\\TFRS\\food.com recipes and interactions\\ingr_map.pkl')\n",
    "raw_recipes = pd.read_csv(r'E:\\TFRS\\food.com recipes and interactions\\RAW_recipes.csv')\n",
    "\n",
    "# Transform all the ingredient id strings into lits of integers\n",
    "recipes['ingredient_ids'] = recipes['ingredient_ids'].map(lambda str: [int(ingr_id) for ingr_id in str[1:-1].split(', ')])\n",
    "\n",
    "# Create a cross-reference tables for all recipes and ingredients\n",
    "recipe_ingr_xref = make_recipe_ingr_xref(recipes)\n",
    "\n",
    "# Get the tf-idf embeddings\n",
    "tf_idf_embeddings = get_tf_idf_embeddings(recipe_ingr_xref)\n",
    "\n",
    "# Merge preprocessed and raw recipe tables\n",
    "full_recipes = recipes.merge(raw_recipes, left_on='id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      4\u001b[0m content_recs \u001b[38;5;241m=\u001b[39m similar_recipes(tf_idf_embeddings, recipe_id, k)\n\u001b[1;32m----> 5\u001b[0m collab_recs \u001b[38;5;241m=\u001b[39m similar_recipes(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mitem_factors, recipe_id, k)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you liked \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_recipes[full_recipes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mrecipe_id]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, you might also like:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, rec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(content_recs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "recipe_id = 13\n",
    "k = 10\n",
    "\n",
    "content_recs = similar_recipes(tf_idf_embeddings, recipe_id, k)\n",
    "collab_recs = similar_recipes(model.item_factors, recipe_id, k)\n",
    "\n",
    "print(f'If you liked {full_recipes[full_recipes[\"i\"] == recipe_id].iloc[0][\"name\"]}, you might also like:')\n",
    "for i, rec in enumerate(content_recs):\n",
    "    print(full_recipes[full_recipes['i'] == rec].iloc[0]['name'])\n",
    "    \n",
    "print()    \n",
    "\n",
    "print(f'Customers who liked {full_recipes[full_recipes[\"i\"] == recipe_id].iloc[0][\"name\"]} also liked:')\n",
    "for i, rec in enumerate(collab_recs):\n",
    "    print(full_recipes[full_recipes['i'] == rec].iloc[0]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Old-style content-based reccomendations\n",
    "\n",
    "# Pull so many more similar recipes to a given recipe\n",
    "# We can't compute the entire similarity matrix: it's too much data\n",
    "# Seems to work reasonably well, though, based on recipe names!\n",
    "'''\n",
    "TEST_RECIPE = 0\n",
    "NUM_RECIPES = 10 # (the first one is the recipe itself)\n",
    "\n",
    "similarities = cosine_similarity(embeddings, embeddings[TEST_RECIPE])\n",
    "most_similar = np.argsort(similarities.squeeze())[-NUM_RECIPES:][::-1]\n",
    "print(most_similar)\n",
    "raw_recipes[raw_recipes['id'].isin(recipes[recipes.index.isin(most_similar)]['id'].values)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Model (Matrix Completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parameter search across iterations revealed that a model with 8 factors, 8 alpha performed much better at 30 iterations than at 15 (relatively speaking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following 2 cells to get all functions for collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninteraction_matrix = np.array([[0, 0, 1, 0, 0, 1, 1, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0]]).T\\npositive_interactions = interaction_matrix.sum(axis=0)\\n\\nuser_id = 0\\nrecs = [2, 3, 4, 5, 6]\\n\\naverage_precision(interaction_matrix, positive_interactions, user_id, recs) # Should be 0.7 if everything is correct\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Various metrics for evaluating collaborative filtering models\n",
    "\n",
    "Metric implementations based in part from: https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "Formulas from: http://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html\n",
    "'''\n",
    "\n",
    "# Individual user metrics\n",
    "###########################\n",
    "\n",
    "def precision_at_k(interaction_matrix, positive_interactions, user_id, recs):\n",
    "    '''\n",
    "    interaction_matrix is a matrix of shape (items, users) that you wish to evaluate reccomendations on\n",
    "    positive_interactions should be a Pandas Series mapping user_id to the number positive interactions they have in the given matrix\n",
    "    user_id is the integer of the user to evaluate\n",
    "    recs is a list of k recipes ids that are being reccomended, in order\n",
    "    \n",
    "    Returns the precision@k: the proportion of reccomendations made that are relevant out of total reccomendations\n",
    "    '''\n",
    "    \n",
    "    relevant_reccomendations = interaction_matrix[recs, user_id].sum()\n",
    "    \n",
    "    return relevant_reccomendations / len(recs)\n",
    "\n",
    "def adjusted_precision_at_k(interaction_matrix, positive_interactions, user_id, recs):\n",
    "    '''\n",
    "    Similar to precision_at_k, but will adjust the precision if there are fewer relevant items than reccomendations made\n",
    "    '''\n",
    "\n",
    "    relevant_reccomendations = interaction_matrix[recs, user_id].sum()\n",
    "    max_possible_relevant = min(len(recs), positive_interactions[user_id])\n",
    "    \n",
    "    return relevant_reccomendations / max_possible_relevant\n",
    "    \n",
    "def average_precision(interaction_matrix, positive_interactions, user_id, recs, adjusted=False):\n",
    "    '''\n",
    "    Parameters defined as above\n",
    "    \n",
    "    adjusted determines whether or not to use adjust precision@k when computing average precision\n",
    "    '''\n",
    "    \n",
    "    precision_func = adjusted_precision_at_k if adjusted else precision_at_k\n",
    "    \n",
    "    precisions = [precision_func(interaction_matrix, positive_interactions, user_id, recs[:k+1]) for k in range(len(recs)) if interaction_matrix[recs[k], user_id] == 1]\n",
    "    \n",
    "    return (1 / positive_interactions[user_id]) * sum(precisions)\n",
    "\n",
    "# Aggregate metrics\n",
    "######################\n",
    "\n",
    "def mean_precision_at_k(interaction_matrix, positive_interactions, rec_matrix, adjusted=False):\n",
    "    '''\n",
    "    rec_matrix is a matrix of shape (users, reccomendations) with item_ids recomended for each user\n",
    "    \n",
    "    Computes the mean precision at k across all users\n",
    "    '''\n",
    "    \n",
    "    precision_func = adjusted_precision_at_k if adjusted else precision_at_k\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    for user_id in tqdm.tqdm(range(interaction_matrix.shape[1])):\n",
    "        total += precision_func(interaction_matrix, positive_interactions, user_id, rec_matrix[user_id])\n",
    "    \n",
    "    return total / interaction_matrix.shape[1]\n",
    "\n",
    "def mean_average_precision(interaction_matrix, positive_interactions, rec_matrix, adjusted=False):\n",
    "    '''\n",
    "    Computes the mean average precision across all users (excuse the double \"mean\")\n",
    "    '''\n",
    "\n",
    "    total = 0\n",
    "    \n",
    "    for user_id in tqdm.tqdm(range(interaction_matrix.shape[1])):\n",
    "        total += average_precision(interaction_matrix, positive_interactions, user_id, rec_matrix[user_id], adjusted=adjusted)\n",
    "    \n",
    "    return total / interaction_matrix.shape[1]\n",
    "    \n",
    "'''\n",
    "interaction_matrix = np.array([[0, 0, 1, 0, 0, 1, 1, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0]]).T\n",
    "positive_interactions = interaction_matrix.sum(axis=0)\n",
    "\n",
    "user_id = 0\n",
    "recs = [2, 3, 4, 5, 6]\n",
    "\n",
    "average_precision(interaction_matrix, positive_interactions, user_id, recs) # Should be 0.7 if everything is correct\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c9e89182ce4801a2dfbcff2303007c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25076 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32560\\1788287032.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# Basically, this is a hacky solution, but I think it's the best way to do it (without eliminating massive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;31m# swathes of data and creating a complicated dual-class stratification algorithm which probably won't even\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m# end up working because users and recipes probably aren't distributed nicely)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mtrain_interactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_interactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'u'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'u'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'i'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rating'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[0mtest_interactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_interactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'u'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'u'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'i'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rating'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mtrain_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_interactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_interactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_interactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'u'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\TFRS\\tfrs\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data loading and splitting\n",
    "'''\n",
    "\n",
    "# NOTE: these splits do not all represent all users. Training data does, but test and validation are missing some users\n",
    "# Also note there is a very large range in interaction number: between 2 and 6000 reviews per person (25% are just 2)\n",
    "df1 = pd.read_csv(r'E:\\TFRS\\food.com recipes and interactions\\interactions_train.csv')\n",
    "df2 = pd.read_csv(r'E:\\TFRS\\food.com recipes and interactions\\interactions_test.csv')\n",
    "df3 = pd.read_csv(r'E:\\TFRS\\food.com recipes and interactions\\interactions_validation.csv')\n",
    "\n",
    "# Combine the interaction data together, so that we can create our own splits\n",
    "# 718379 interactions (raw)\n",
    "df = pd.concat((df1, df2, df3), ignore_index=True)\n",
    "\n",
    "# Dropping all the interactions that are less than 3\n",
    "# This complicates things, because we need to reset the user ids\n",
    "# I think this is something to do in the future, but will require some more work\n",
    "'''\n",
    "# Dropping all ratings that are less than 3 (consider those negative interations)\n",
    "# 689321 interactions (losing 29,000 interactions)\n",
    "df.drop(df[df['rating'] < 3].index, inplace=True)\n",
    "\n",
    "# Dropping all users that now have less than 2 interactions (lose 552 users, 552 interactions)\n",
    "# 688769 interactions\n",
    "too_few = df.groupby('u')['rating'].count() < 2 # Dataframe storing True/False for each user having too few ratings\n",
    "too_few_users = too_few.index[too_few] # Stores a list of user ids with too few ratings\n",
    "\n",
    "df.drop(df[df['u'].isin(too_few_users)].index, inplace=True)\n",
    "\n",
    "# Resulting interaction matrix is 24392 x 173600 (99.98% sparse, which does seem high...)\n",
    "# Can consider eliminating some recipes with fewer than 1 rating (but that is 80601 recipes, nearly 50%)\n",
    "'''\n",
    "\n",
    "# In theory, we could do k-fold cross validation, but then we'd need to have k interactions per user\n",
    "# So I think deciding to change the minimum number of reviews is a precondition to even attempting k-fold cross validation\n",
    "\n",
    "user_train, user_test, item_train, item_test, =\\\n",
    "    safe_train_test_split(df['u'], df['i'], test_size=0.25, random_state=1)\n",
    "\n",
    "# Total users: 25075\n",
    "# Total items: 178264\n",
    "\n",
    "# Build the new dataframes from the train-test split\n",
    "train_interactions = pd.DataFrame({'u': user_train, 'i': item_train, 'rating': 1})\n",
    "test_interactions = pd.DataFrame({'u': user_test, 'i': item_test, 'rating': 1})\n",
    "\n",
    "# Store the number of positive interactions associated with each user to avoid recomputing for precision@k metrics\n",
    "positive_train_interactions = train_interactions.groupby('u')['rating'].count()\n",
    "positive_test_interactions = test_interactions.groupby('u')['rating'].count()\n",
    "\n",
    "# Supplement both with a 0 rating at the max user and recipe id to make matrices the same size\n",
    "# This is a dumb solution to the problem of some recipes only appearing once\n",
    "# I can only stratify across one thing anyways, so I think we're going to have problems with some recipes\n",
    "# having a learned embedding. The good news is the metrics will be more user-focused, so it should be OK\n",
    "\n",
    "# Basically, this is a hacky solution, but I think it's the best way to do it (without eliminating massive\n",
    "# swathes of data and creating a complicated dual-class stratification algorithm which probably won't even\n",
    "# end up working because users and recipes probably aren't distributed nicely)\n",
    "\n",
    "train_interactions = train_interactions.append({'u': max(df['u']), 'i': max(df['i']), 'rating': 0}, ignore_index=True)\n",
    "test_interactions = test_interactions.append({'u': max(df['u']), 'i': max(df['i']), 'rating': 0}, ignore_index=True)\n",
    "\n",
    "train_matrix = csr_matrix((np.ones(len(train_interactions)), (train_interactions['i'], train_interactions['u'])))\n",
    "test_matrix = csr_matrix((np.ones(len(test_interactions)), (test_interactions['i'], test_interactions['u'])))\n",
    "complete_matrix = train_matrix + test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: mean precision takes a lot longer when you include 100 reccomendations\n",
    "'''\n",
    "train_p_at_k = mean_precision_at_k(train_matrix, positive_train_interactions, train_rec_matrix[:, :10], adjusted=False)\n",
    "test_p_at_k = mean_precision_at_k(test_matrix, positive_test_interactions, test_rec_matrix[:, :10], adjusted=False)\n",
    "\n",
    "train_adj_p_at_k = mean_precision_at_k(train_matrix, positive_train_interactions, train_rec_matrix[:, :10], adjusted=True)\n",
    "test_adj_p_at_k = mean_precision_at_k(test_matrix, positive_test_interactions, test_rec_matrix[:, :10], adjusted=True)\n",
    "\n",
    "train_mean_p = mean_average_precision(train_matrix, positive_train_interactions, train_rec_matrix[:, :10], adjusted=False)\n",
    "test_mean_p = mean_average_precision(test_matrix, positive_test_interactions, test_rec_matrix[:, :10], adjusted=False)\n",
    "\n",
    "train_mean_p_adj = mean_average_precision(train_matrix, positive_train_interactions, train_rec_matrix[:, :10], adjusted=True)\n",
    "test_mean_p_adj = mean_average_precision(test_matrix, positive_test_interactions, test_rec_matrix[:, :10], adjusted=True)\n",
    "\n",
    "rand_p_at_k = mean_precision_at_k(test_matrix, positive_test_interactions, random_matrix[:, :10], adjusted=False)\n",
    "rand_adj_p_at_k = mean_precision_at_k(test_matrix, positive_test_interactions, random_matrix[:, :10], adjusted=True)\n",
    "rand_mean_p = mean_average_precision(test_matrix, positive_test_interactions, random_matrix[:, :10], adjusted=False)\n",
    "rand_mean_p_adj = mean_average_precision(test_matrix, positive_test_interactions, random_matrix[:, :10], adjusted=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "train_rec_matrix, test_rec_matrix = train_test_rec_matrix(model.item_factors, model.user_factors, 100, train_interactions)\n",
    "random_matrix = random_rec_matrix(train_matrix, 100)\n",
    "\n",
    "assert mean_precision_at_k(train_matrix, positive_train_interactions, test_rec_matrix, adjusted=False) == 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print(train_p_at_k, test_p_at_k, rand_p_at_k)\n",
    "print(train_adj_p_at_k,test_adj_p_at_k, rand_adj_p_at_k)\n",
    "print(train_mean_p, test_mean_p, rand_mean_p)\n",
    "print(train_mean_p_adj, test_mean_p_adj, rand_mean_p_adj)\n",
    "'''\n",
    "'''\n",
    "Pre proper filtering (10 recs):\n",
    "0.09332828202262554 0.006751475514435921 2.7915137980539158e-05\n",
    "0.24018909499248356 0.024444925521652214 4.320199925559632e-05\n",
    "0.1406715379458244 0.00753182893600831 5.866304081570052e-06\n",
    "0.17384917444761513 0.0185085742058478 7.638693794620157e-06\n",
    "\n",
    "Proper filtering @10 recs\n",
    "0.09339208805229554 0.00802759610783195 4.386664539799011e-05\n",
    "0.2378505058906556 0.02794409756245771 7.975753708725475e-05\n",
    "0.13841172806680735 0.009886528168548337 2.1529499979817866e-05\n",
    "0.17088549890951885 0.021114400425549178 4.146888425163155e-05\n",
    "\n",
    "Proper filtering @100 recs\n",
    "0.02811652576167531 0.003499361939703625 4.426543308342641e-05\n",
    "0.3901281604724576 0.09905876639766126 0.0007262624622543001\n",
    "0.15152008275914267 0.012511155940690546 3.548401140167055e-05\n",
    "0.25815126573941893 0.07473983983342303 0.0004636069582234532\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_results_dict():\n",
    "    return {\n",
    "        'factors': [],\n",
    "        'regularization': [],\n",
    "        'alpha': [],\n",
    "        'iterations': [],\n",
    "        'train_p_at_k': [],\n",
    "        'test_p_at_k': [],\n",
    "        'train_mean_p': [],\n",
    "        'test_mean_p': []\n",
    "    }\n",
    "\n",
    "def compute_metrics(model, step, k, train_interactions, factors, regularization, alpha, results):\n",
    "    def compute(iteration, time):\n",
    "        if (iteration + 1) % step == 0:\n",
    "            train_rec_matrix, test_rec_matrix = collab_filter_train_test_matrix(model.item_factors, model.user_factors, k, train_interactions)\n",
    "            \n",
    "            train_adj_p_at_k = mean_precision_at_k(train_matrix, positive_train_interactions, train_rec_matrix, adjusted=True)\n",
    "            test_adj_p_at_k = mean_precision_at_k(test_matrix, positive_test_interactions, test_rec_matrix, adjusted=True)\n",
    "\n",
    "            train_mean_p_adj = mean_average_precision(train_matrix, positive_train_interactions, train_rec_matrix, adjusted=True)\n",
    "            test_mean_p_adj = mean_average_precision(test_matrix, positive_test_interactions, test_rec_matrix, adjusted=True)\n",
    "            \n",
    "            results['factors'].append(factors)\n",
    "            results['regularization'].append(regularization)\n",
    "            results['alpha'].append(alpha)\n",
    "            results['iterations'].append(iteration+1)\n",
    "            \n",
    "            results['train_p_at_k'].append(train_adj_p_at_k)\n",
    "            results['test_p_at_k'].append(test_adj_p_at_k)\n",
    "            results['train_mean_p'].append(train_mean_p_adj)\n",
    "            results['test_mean_p'].append(test_mean_p_adj)\n",
    "            \n",
    "            print(f'Iteration: {iteration + 1}')\n",
    "            print(f'Adjusted precision@k: Train: {train_adj_p_at_k}, Test: {test_adj_p_at_k}')\n",
    "            print(f'Adjusted mean precision: Train: {train_mean_p_adj}, Test: {test_mean_p_adj}')\n",
    "            print()\n",
    "    \n",
    "    return compute\n",
    "\n",
    "def train_model(factors, regularization, alpha, iterations, metric_steps, train_interactions, results, k=10, random_state=0):\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=factors, iterations=iterations, regularization=regularization, random_state=random_state)\n",
    "    \n",
    "    model.fit_callback = compute_metrics(model, metric_steps, k, train_interactions, factors, regularization, alpha, results)\n",
    "    \n",
    "    model.fit(alpha * train_matrix)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "factors = [8]\n",
    "regularizations = [0.1]\n",
    "alphas = [64]\n",
    "iteration_step = 30\n",
    "max_iterations = 30\n",
    "\n",
    "k = 10\n",
    "\n",
    "results = make_results_dict()\n",
    "\n",
    "for factor, regularization, alpha in product(factors, regularizations, alphas):\n",
    "    print(f'Factors: {factor}, regularization: {regularization}, alpha: {alpha}')\n",
    "    \n",
    "    model = train_model(factor, regularization, alpha, max_iterations, iteration_step, train_interactions, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A bunch of functions that I don't think I need anymore, but am keeping just in case\n",
    "\n",
    "Implementations for collaborative filtering reccomendation generation that compute things in blocks.\n",
    "This was an attempt to speed up reccomendation generation b/c it allows for faster matrix multiplication.\n",
    "However, getting the top k elements takes far longer, so the improvement is margianl on CPU, and non-existent on GPU\n",
    "\n",
    "Also, some older implemenetations of reccomendation generation\n",
    "'''\n",
    "\n",
    "def get_block_predictions(item_matrix, user_matrix, start_user_id, step):\n",
    "    return (item_matrix @ user_matrix[start_user_id:start_user_id + step, :].T).T\n",
    "\n",
    "def get_block_reccomendations(predictions, k):\n",
    "    # Get the top k reccoemndations for each user (row) in the prediction matrix provided\n",
    "    unsorted_recs = np.argpartition(predictions, -k)[:, -k:]\n",
    "    \n",
    "    # Pull the predicted values for those reccomendations\n",
    "    rec_values = np.take_along_axis(predictions, unsorted_recs, axis=1)\n",
    "    \n",
    "    # Get the indices to sort the reccoemndation by predicted values\n",
    "    rec_order = np.argsort(rec_values)[:, ::-1]\n",
    "    \n",
    "    # Use the sorted indices to get the reccomendations in sorted order\n",
    "    sorted_recs = np.take_along_axis(unsorted_recs, rec_order, axis=1)\n",
    "    \n",
    "    return sorted_recs\n",
    "\n",
    "def get_block_rec_matrix(item_matrix, user_matrix, k, block_size=4):\n",
    "    rec_matrix = []\n",
    "    \n",
    "    for start_user_id in tqdm.tqdm(range(0, user_matrix.shape[0], block_size)):\n",
    "        predictions = get_block_predictions(item_matrix, user_matrix, start_user_id, block_size)\n",
    "        recs = get_block_reccomendations(predictions, k, train_interactions, filter_recs)\n",
    "        \n",
    "        rec_matrix.append(recs)\n",
    "\n",
    "    return np.vstack(rec_matrix)\n",
    "\n",
    "def train_test_block_rec_matrix(item_matrix, user_matrix, k, train_interactions, block_size=4):\n",
    "    train_rec_matrix = []\n",
    "    test_rec_matrix = []\n",
    "    \n",
    "    for start_user_id in tqdm.tqdm(range(0, user_matrix.shape[0], block_size)):\n",
    "        # Get the predictions without filtering out known-positives from the test set\n",
    "        predictions = get_block_predictions(item_matrix, user_matrix, start_user_id, block_size)\n",
    "        train_recs = get_block_reccomendations(predictions, k)\n",
    "        \n",
    "        # Get the ids of the interactions to filter out\n",
    "        filter_idxs = np.array([train_interactions[train_interactions['u'] == user_id]['i'].values for user_id in range(start_user_id, start_user_id+block_size)])\n",
    "        print(filter_idxs)\n",
    "        \n",
    "        # Give the reccomendations from the train set low values to prevent them from being reccomended\n",
    "        zeros = np.array([np.full_like(f, 0) for f in filter_idxs])\n",
    "        test_predictions = np.put_along_axis(predictions, filter_idxs, zeros, axis=1)\n",
    "        \n",
    "        # Regenerate reccomendations with the filtered predictions\n",
    "        test_recs = get_block_reccomendations(test_predictions, k)\n",
    "        \n",
    "        train_rec_matrix.append(train_recs)\n",
    "        test_rec_matrix.append(test_recs)\n",
    "\n",
    "    return np.vstack(train_rec_matrix), np.vtack(test_rec_matrix)\n",
    "\n",
    "def custom_create_rec_matrix(item_matrix, user_matrix, k, train_interactions=None, filter_recs=False):\n",
    "    '''\n",
    "    Returns a numpy array of size (users, k) with the top k reccomendations for each user\n",
    "    '''\n",
    "\n",
    "    rec_matrix = []\n",
    "    \n",
    "    for user_id in tqdm.tqdm(range(user_matrix.shape[0])):\n",
    "        recs = get_reccomendations(item_matrix, user_matrix, user_id, k, train_interactions, filter_recs)[0]\n",
    "        \n",
    "        rec_matrix.append(recs)\n",
    "\n",
    "    return np.array(rec_matrix)\n",
    "\n",
    "def create_rec_matrix(model, train_matrix, k, test_mode=False):\n",
    "    '''\n",
    "    model is implicit reccomendation model\n",
    "    train_matrix is the interaction matrix the model trained on\n",
    "    k is the number of reccomendations to generate per user\n",
    "    test mode: if true, will filter out recomendations from the train matrix\n",
    "    '''\n",
    "    \n",
    "    rec_matrix = []\n",
    "    \n",
    "    for user_id in tqdm.tqdm(range(train_matrix.shape[1])):\n",
    "        recs = [rec[0] for rec in model.recommend(user_id, train_matrix.T, k, filter_already_liked_items=test_mode)]\n",
    "        \n",
    "        rec_matrix.append(recs)\n",
    "\n",
    "    return np.array(rec_matrix)\n",
    "\n",
    "def get_reccomendations(item_matrix, user_matrix, user_id, k, train_interactions=None, filter_recs=False):\n",
    "    '''\n",
    "    item matrix is of shape (items, features) describing the features for each item\n",
    "    user_matrix is of shape (users, features) describing the features for each user\n",
    "    user_id is an integer, the user to get reccomendations for\n",
    "    train_interactions is a pandas dataframe describing all positive interactions the model trained on (faster than interaction matrix)\n",
    "    filter determines whether or not to eliminate reccomendations that have a positive interaction in the train_matrix\n",
    "    \n",
    "    Returns a tuple, (reccomendations, values) where reccomendations is a list of length k with reccomendation indices, and values gives \n",
    "    \n",
    "    Note: this was created because implicit's built in reccomend function is slow, and doesn't filter out the training reccomendations\n",
    "    '''\n",
    "    \n",
    "    # Get all predicted values for the user\n",
    "    predictions = item_matrix @ user_matrix[user_id, :]\n",
    "    \n",
    "    if filter_recs:\n",
    "        #trained_recs_a = np.nonzero(train_matrix[:, user_id])[0]\n",
    "        trained_recs = train_interactions[train_interactions['u'] == user_id]['i'].values\n",
    "        \n",
    "        predictions[trained_recs] = 0\n",
    "    \n",
    "    # Get the top k reccomendations for the user(NOTE: this function does not return them in sorted order)\n",
    "    unsorted_recs = np.argpartition(predictions, -k)[-k:]\n",
    "    \n",
    "    # Sort the reccomendations produced by their values, in descending order (that's how the metrics expect them)\n",
    "    sorted_recs = sorted(unsorted_recs, key=lambda rec: predictions[rec], reverse=True)\n",
    "    \n",
    "    # Get the values associated with these reccomendations\n",
    "    sorted_values = predictions[sorted_recs]\n",
    "    \n",
    "    return sorted_recs, sorted_values\n",
    "    \n",
    "def train_test_reccomendations(item_matrix, user_matrix, user_id, k, train_interactions):\n",
    "    '''\n",
    "    Produces both train and test reccomendations for a user at once, which should be more efficient\n",
    "    '''\n",
    "    \n",
    "    # Get all predicted values for the user\n",
    "    predictions = item_matrix @ user_matrix[user_id, :]\n",
    "    \n",
    "    # Get unfiltered reccomendations\n",
    "    unsorted_recs = np.argpartition(predictions, -k)[-k:]\n",
    "    sorted_recs = sorted(unsorted_recs, key=lambda rec: predictions[rec], reverse=True)\n",
    "    \n",
    "    # Filter the predictions\n",
    "    trained_recs = train_interactions[train_interactions['u'] == user_id]['i'].values\n",
    "    predictions[trained_recs] = 0\n",
    "        \n",
    "    # Get the filtered predictions\n",
    "    unsorted_filtered_recs = np.argpartition(predictions, -k)[-k:]\n",
    "    sorted_filtered_recs = sorted(unsorted_filtered_recs, key=lambda rec: predictions[rec], reverse=True)\n",
    "    \n",
    "    return sorted_recs, sorted_filtered_recs\n",
    "    \n",
    "def train_test_rec_matrix(item_matrix, user_matrix, k, train_interactions):\n",
    "    '''\n",
    "    Produces a train and test reccomendation matrix\n",
    "    \n",
    "    This is about 30% faster than generating them seperately\n",
    "    '''\n",
    "    \n",
    "    train_rec_matrix = []\n",
    "    test_rec_matrix = []\n",
    "    \n",
    "    for user_id in tqdm.tqdm(range(user_matrix.shape[0])):\n",
    "        train_recs, test_recs = train_test_reccomendations(item_matrix, user_matrix, user_id, k, train_interactions)\n",
    "        \n",
    "        train_rec_matrix.append(train_recs)\n",
    "        test_rec_matrix.append(test_recs)\n",
    "\n",
    "    return np.array(train_rec_matrix), np.array(test_rec_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(results)\n",
    "#df2.groupby(['factors', 'regularization', 'alpha']).max()\n",
    "df = pd.concat((df, df1))\n",
    "df.to_csv('param_search1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['factors', 'regularization', 'alpha']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Non-negative matrix factorization\n",
    "# I don't NMF is what we want. The matrix is far too sparse, and this was too slow\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Ratings matrix\n",
    "# 16 components: 257 iterations, 3790.7093189610523 error, 0.0275 reconstructed\n",
    "# 32 components: 87 iterations, 3707.651615877227 error, 0.0917 reconstructed\n",
    "# 64 components: 172 iterations, 3583.3304672483646 error, 0.1284 reconstructed\n",
    "# 128 components: 137 iterations, 3404.455889599025 error, 0.1835 reconstructed, found a new reccoemendation\n",
    "#      (very slow, 20+ minutes, violation started going up at about 100 iterations in)\n",
    "# 256 components: 237 iterations, 3167 error, 0.3119 reconstructed\n",
    "#      (very, very slow, 3+ hours, violation also went up at about 100 iterations)\n",
    "\n",
    "model = NMF(n_components=256, random_state=0, max_iter=500, verbose=1)\n",
    "\n",
    "W = model.fit_transform(ratings_matrix)\n",
    "\n",
    "H = model.components_\n",
    "\n",
    "print(model.n_iter_)\n",
    "print(model.reconstruction_err_)\n",
    "\n",
    "good = 0\n",
    "total = 0\n",
    "for i in range(0, 100):\n",
    "    for j in range(0, 100):\n",
    "        if ratings_matrix[i, j] != 0:\n",
    "            total += 1\n",
    "            print(i, j, ratings_matrix[i, j], W[i] @ H[:, j])\n",
    "            if W[i] @ H[:, j] > 1:\n",
    "                good += 1\n",
    "        elif W[i] @ H[:, j] > 1:\n",
    "            print('NEW ONE!')\n",
    "print(good / total)\n",
    "print(good, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Matrix Factorization in Keras: requires sampling from negative interactions, handling balance\n",
    "# Overfit like crazy with the metrics I had selected (unsure how that compares to other metrics)\n",
    "# Does allow for custom, tunable objective functions\n",
    "# Is the direction we may go in with neural collaborative filtering...\n",
    "\n",
    "def mf_model(num_users, num_recipes, rank):\n",
    "    user_input = keras.Input((1, ))\n",
    "    recipe_input = keras.Input((1, ))\n",
    "    \n",
    "    user_embedding = layers.Embedding(num_users, rank)(user_input)\n",
    "    recipe_embedding = layers.Embedding(num_recipes, rank)(recipe_input)\n",
    "    \n",
    "    dot_product = layers.Dot(axes=(2))([user_embedding, recipe_embedding])\n",
    "    \n",
    "    return keras.Model(inputs=[user_input, recipe_input], outputs=dot_product)\n",
    "\n",
    "# A rank of 5 stopped the validation loss from going below 19.5421 (which is really bad)\n",
    "# Rank 32: increase trainable paramters to 6 million. Validation loss is still awful\n",
    "# Increasing batch size to 512 sped things up, unsure on performance differences\n",
    "\n",
    "# Rank 16, still overfitting very strongly\n",
    "\n",
    "model = mf_model(max(df['u'] + 1), max(df['i'] + 1), 16)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "history = model.fit(x=X_train, y=y_train, batch_size=512, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parameter search for implicit ALS weighted regularized matrix factorization (WRMF)\n",
    "# This method is the most hopeful of any of the methods explores thus far\n",
    "\n",
    "import itertools\n",
    "\n",
    "def get_rmse(y_true, y_w, y_h, block_size=4):\n",
    "    total_rmse = 0\n",
    "    \n",
    "    for i in tqdm.tqdm(range(0, y_true.shape[0], block_size)):\n",
    "        total_rmse += mean_squared_error(y_true[:, i:i + block_size].toarray(), y_w @ y_h[i:i + block_size].T, squared=False)\n",
    "\n",
    "    total_rmse /= (binary_matrix.shape[0] / block_size)\n",
    "    \n",
    "    return total_rmse\n",
    "    \n",
    "alphas = [32, 64, 128]\n",
    "factors = [32, 64, 128]\n",
    "regularizations = [0.1, 1, 10]\n",
    "iterations = 15\n",
    "\n",
    "combos = itertools.product(alphas, factors, regularizations)\n",
    "mse_scores = {}\n",
    "hit_rate_scores = {}\n",
    "\n",
    "for alpha, factor, regularization in combos:\n",
    "    # initialize a model\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=factor, random_state=0, calculate_training_loss=True, iterations=iterations, regularization=regularization)\n",
    "\n",
    "    # train the model on a sparse matrix of item/user/confidence weights\n",
    "    model.fit(alpha * binary_matrix)\n",
    "\n",
    "    mse = get_mse(binary_matrix, model.item_factors, model.user_factors)\n",
    "    \n",
    "    W = model.item_factors\n",
    "    H = model.user_factors\n",
    "\n",
    "    good = 0\n",
    "    total = 0\n",
    "    for i in range(0, 100):\n",
    "        for j in range(0, 100):\n",
    "            if binary_matrix[i, j] != 0:\n",
    "                total += 1\n",
    "                if W[i] @ H[j] > 0.5:\n",
    "                    good += 1\n",
    "                    \n",
    "    hit_rate = good / total\n",
    "    \n",
    "    print(alpha, factor, regularization, mse, hit_rate)\n",
    "    \n",
    "    mse_scores[(alpha, factor, regularization)] = mse\n",
    "    hit_rate_scores[(alpha, factor, regularization)] = hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Embeddings\n",
    "(Didn't seem to work very well, on hold until later)\n",
    "Note: may not have worked well due to swapping the i column, and the index of the recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Following documentation here: https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ingr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simple_ingr_map = ingr_map[['id', 'replaced']].set_index('id').drop_duplicates().to_dict()['replaced']\n",
    "recipes['ingredients'] = recipes['ingredient_ids'].map(lambda ingredient_ids: [simple_ingr_map[ingregient_id] for ingregient_id in ingredient_ids])\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(recipes['ingredients'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=5, window=2, min_count=1, workers=4)\n",
    "model.build_vocab(documents, progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train(documents=documents, total_examples=model.corpus_count, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TEST_RECIPE = 2\n",
    "NUM_RECIPES = 10 # (the first one is the recipe itself)\n",
    "\n",
    "similarities = model.docvecs.distances(TEST_RECIPE)\n",
    "most_similar = np.argsort(similarities.squeeze())[-NUM_RECIPES:]\n",
    "raw_recipes[raw_recipes['id'].isin(recipes[recipes.index.isin(most_similar)]['id'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#reads in all of the datasets \n",
    "recipes = pd.read_csv(r'E:\\TFRS\\food.com recipes and interactions\\PP_recipes.csv')\n",
    "raw_recipes = pd.read_csv(r'E:\\TFRS\\food.com recipes and interactions\\RAW_recipes.csv')\n",
    "raw_interactions = pd.read_csv(r'E:\\TFRS\\food.com recipes and interactions\\RAW_interactions.csv')\n",
    "\n",
    "#combines the raw recipes and numbered recipes into one table\n",
    "#merges them by the recipe id number\n",
    "recipes_info = recipes.merge(raw_recipes, left_on='id', right_on='id')\n",
    "\n",
    "#merges all of the recipe information with the ratings\n",
    "recipes_interact = recipes_info.merge(raw_interactions, left_on='id', right_on='recipe_id')\n",
    "\n",
    "#can easily change which columns we want to output by updating this \n",
    "keep_cols = ['name','id','minutes', 'contributor_id', 'description', 'submitted', 'tags', 'nutrition', 'n_steps', 'ingredient_tokens', 'ingredient_ids', 'steps','steps_tokens', 'user_id', 'date', 'rating', 'review']\n",
    "\n",
    "#prints out the merged table with only the desired columns\n",
    "new_recipes_info = recipes_interact[keep_cols]\n",
    "new_recipes_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfrs",
   "language": "python",
   "name": "tfrs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
